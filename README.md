# Be My Eyes

App made to generate descriptive caption of images, aiming to improve accessibility of visually impared users.

This project was made as final assignment to the discipline of Unstructured Data Mining at University of São Paulo.

Made by:

- Alvaro José Lopes - 10873365
- Natan Henrique Sanches - 11795680

This project used the following technologies:

- [Streamlit](https://docs.streamlit.io/): to create a simple web app where the user can upload an image and retrieve the image caption generated by two possible models (pre-trained and vgg+lstm trained by us), followed by audio description of the caption.
- [FastAPI](https://fastapi.tiangolo.com/): to deploy the models and provide a REST API with an endpoint (`POST /inference/`) to request the image caption generated by a given model.
- [HuggingFace](https://huggingface.co/): to run a image-to-text pipeline and generate captions using a [BLIP pre-trained](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) model.
- [Keras](https://keras.io/): to train a model that uses LSTM to generate captions and uses a pre-trained VGG model as feature extractor of the image. Check the [jupyter notebook used to train](/notebooks/model_training.ipynb).
- [gTTS](https://pypi.org/project/gTTS/): Google Text-to-Speech to generate audio description of images.

## Running the app

Firstly, install all the dependencies

```bash
python3 -m venv venv
source venv/bin/activate
pip3 install -r requirements.txt
```

### Backend

```bash
uvicorn backend:app
```

For default, the backend will be running at port `127.0.0.1:8000`. You can check the autogenerated API docs on `127.0.0.1:8000/docs`

### Frontend

```bash
streamlit run frontend.py
```

For default, the frontend will be running at port `8501`.

## How to use

1. Select one of the available models
2. Upload an image

The app will show the image with the generated caption and provide the audio description.
